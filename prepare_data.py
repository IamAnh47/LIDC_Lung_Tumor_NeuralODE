import os
import yaml
import numpy as np
from tqdm import tqdm
import json
import random
import pylidc as pl  # <--- THÃŠM DÃ’NG NÃ€Y

# Import cÃ¡c module tá»± viáº¿t
from src.data.dicom_loader import DicomLoader
from src.data.preprocessing import normalize_hu, resample_volume, crop_roi
from src.data.generation import mesh_from_mask, generate_sdf_points
from pylidc.utils import consensus


def main():
    # 1. Load Config
    with open("configs/config.yaml", "r") as f:
        cfg = yaml.safe_load(f)

    # Láº¥y Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i
    RAW_DIR = os.path.abspath(cfg['paths']['raw_data'])
    PROCESSED_DIR = os.path.abspath(cfg['paths']['processed_data'])

    # Tham sá»‘ xá»­ lÃ½
    ROI_SIZE = tuple(cfg['data']['roi_size'])
    TARGET_SPACING = tuple(cfg['data']['target_spacing'])
    NUM_SAMPLES = cfg['data']['sdf_samples']

    # Táº¡o thÆ° má»¥c output
    os.makedirs(os.path.join(PROCESSED_DIR, "rois"), exist_ok=True)
    os.makedirs(os.path.join(PROCESSED_DIR, "sdfs"), exist_ok=True)

    # 2. Khá»Ÿi táº¡o DicomLoader
    loader = DicomLoader(RAW_DIR)

    # Láº¥y danh sÃ¡ch bá»‡nh nhÃ¢n
    patient_ids = loader.get_all_patient_ids()
    print(f"ğŸš€ TÃ¬m tháº¥y {len(patient_ids)} bá»‡nh nhÃ¢n. Báº¯t Ä‘áº§u xá»­ lÃ½...")

    processed_records = []

    # Biáº¿n Ä‘áº¿m thá»‘ng kÃª
    stats = {
        "no_nodules": 0,
        "consensus_empty": 0,
        "too_complex": 0,
        "too_small": 0,
        "success": 0
    }

    for pid in tqdm(patient_ids):
        try:
            # Query Ä‘á»ƒ check xem cÃ³ nodule khÃ´ng trÆ°á»›c khi load áº£nh náº·ng
            # (DÃ¹ng loader.load_patient_data Ä‘Ã£ bao gá»“m bÆ°á»›c nÃ y, nhÆ°ng tÃ¡ch ra Ä‘á»ƒ Ä‘áº¿m stats chuáº©n hÆ¡n)
            scan = pl.query(pl.Scan).filter(pl.Scan.patient_id == pid).first()
            if not scan: continue

            nodules = scan.cluster_annotations()
            if not nodules:
                stats["no_nodules"] += 1
                continue

            # DÃ¹ng Loader Ä‘á»ƒ láº¥y dá»¯ liá»‡u thÃ´ (áº¢nh + Spacing)
            # LÆ°u Ã½: HÃ m load_patient_data trong dicom_loader.py tráº£ vá» 3 giÃ¡ trá»‹
            vol_orig, spacing_orig, nodules = loader.load_patient_data(pid)

            if vol_orig is None:
                continue

            # Xá»­ lÃ½ tá»«ng khá»‘i u
            for i, annots in enumerate(nodules):
                # --- Lá»ŒC Lá»–I 1: QUÃ PHá»¨C Táº P ---
                if len(annots) > 4:
                    # print(f"   âš ï¸ Bá» qua Nodule {i} cá»§a {pid}: >4 anns.")
                    stats["too_complex"] += 1
                    continue
                # ==============================================================================
                # [THAM KHáº¢O SAU NÃ€Y] Bá»˜ Lá»ŒC Äá»˜ ÃC TÃNH (MALIGNANCY FILTER)
                # ------------------------------------------------------------------------------
                # Má»—i annotation cÃ³ thuá»™c tÃ­nh .malignancy (1: LÃ nh tÃ­nh -> 5: Ãc tÃ­nh)
                # Ta tÃ­nh trung bÃ¬nh cá»™ng Ä‘Ã¡nh giÃ¡ cá»§a cÃ¡c bÃ¡c sÄ©.
                #
                # avg_malignancy = np.mean([a.malignancy for a in annots])
                #
                # if avg_malignancy < 3:
                #     # print(f"   â© Bá» qua Nodule {i}: Kháº£ nÄƒng cao lÃ  lÃ nh tÃ­nh (Score: {avg_malignancy:.1f})")
                #     continue
                # ==============================================================================
                # 1. Consensus Mask
                try:
                    mask_orig, cbbox, _ = consensus(annots, clevel=0.5, pad=10)
                except Exception:
                    stats["consensus_empty"] += 1
                    continue

                # --- Lá»ŒC Lá»–I 2: MASK QUÃ BÃ‰ ---
                if np.sum(mask_orig) < 50:
                    stats["consensus_empty"] += 1
                    continue

                # Crop volume gá»‘c theo bbox cá»§a mask
                # cbbox lÃ  tuple cá»§a cÃ¡c slice objects
                vol_nodule = vol_orig[cbbox]

                # 2. Resample & Normalize
                vol_res, mask_res = resample_volume(vol_nodule, mask_orig, spacing_orig, TARGET_SPACING)
                vol_norm = normalize_hu(vol_res)

                # 3. Crop Fixed ROI
                roi_vol, roi_mask = crop_roi(vol_norm, mask_res, size=ROI_SIZE)

                if roi_vol is None: continue

                # 4. Generate Mesh & Check Size
                mesh = mesh_from_mask(roi_mask, spacing=TARGET_SPACING)

                # --- Lá»ŒC Lá»–I 3: MESH Lá»–I ---
                if mesh is None or len(mesh.vertices) < 10:
                    stats["too_small"] += 1
                    continue

                # 5. Generate SDF Data (DÃ¹ng láº¡i mesh Ä‘Ã£ táº¡o)
                points, sdfs = generate_sdf_points(mesh, num_samples=NUM_SAMPLES, roi_size=ROI_SIZE)

                if points is None: continue

                # 6. Save Disk
                file_id = f"{pid}_nodule{i}"

                np.save(os.path.join(PROCESSED_DIR, "rois", f"{file_id}.npy"), roi_vol)
                np.savez(os.path.join(PROCESSED_DIR, "sdfs", f"{file_id}.npz"), points=points, sdfs=sdfs)

                stats["success"] += 1
                processed_records.append(file_id)

        except Exception as e:
            print(f"âš ï¸ Lá»—i {pid}: {e}")
            continue

    print("\nğŸ“Š BÃO CÃO CHI TIáº¾T:")
    print(f"âŒ KhÃ´ng cÃ³ nodule: {stats['no_nodules']}")
    print(f"âŒ Mask rá»—ng/bÃ©: {stats['consensus_empty']}")
    print(f"âŒ QuÃ¡ phá»©c táº¡p: {stats['too_complex']}")
    print(f"âŒ Mesh lá»—i: {stats['too_small']}")
    print(f"âœ… THÃ€NH CÃ”NG: {stats['success']} máº«u")

    # 4. Chia táº­p Train/Val/Test
    if len(processed_records) > 0:
        print(f"\nğŸ“¦ Tá»•ng cá»™ng: {len(processed_records)} máº«u sáº¡ch.")
        print("âœ‚ï¸ Äang chia táº­p dá»¯ liá»‡u...")

        random.seed(42)
        random.shuffle(processed_records)

        n_total = len(processed_records)
        n_train = int(n_total * 0.7)
        n_val = int(n_total * 0.1)

        split_dict = {
            "train": processed_records[:n_train],
            "val": processed_records[n_train:n_train + n_val],
            "test": processed_records[n_train + n_val:]
        }

        with open(os.path.join(PROCESSED_DIR, "split_data.json"), "w") as f:
            json.dump(split_dict, f, indent=4)

        print(
            f"âœ… ÄÃ£ lÆ°u split_data.json (Train: {len(split_dict['train'])}, Val: {len(split_dict['val'])}, Test: {len(split_dict['test'])})")
    else:
        print("âŒ KhÃ´ng cÃ³ dá»¯ liá»‡u nÃ o Ä‘Æ°á»£c xá»­ lÃ½ thÃ nh cÃ´ng.")


if __name__ == "__main__":
    main()