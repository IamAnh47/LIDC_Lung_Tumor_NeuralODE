import os
import yaml
import numpy as np
from tqdm import tqdm
import json
import random
import pylidc as pl
import glob

# Import c√°c module t·ª± vi·∫øt
from src.data.dicom_loader import DicomLoader
from src.data.preprocessing import normalize_hu, resample_volume, crop_roi
from src.data.generation import mesh_from_mask, generate_sdf_points
from pylidc.utils import consensus


def main():
    # 1. Load Config
    config_path = "configs/config.yaml"
    if not os.path.exists(config_path):
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file config t·∫°i: {config_path}")
        return

    with open(config_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    # Setup ƒë∆∞·ªùng d·∫´n
    RAW_DIR = os.path.abspath(cfg['paths']['raw_data'])
    PROCESSED_DIR = os.path.abspath(cfg['paths']['processed_data'])
    ROI_DIR = os.path.join(PROCESSED_DIR, "rois")
    SDF_DIR = os.path.join(PROCESSED_DIR, "sdfs")

    os.makedirs(ROI_DIR, exist_ok=True)
    os.makedirs(SDF_DIR, exist_ok=True)

    ROI_SIZE = tuple(cfg['data']['roi_size'])
    TARGET_SPACING = tuple(cfg['data']['target_spacing'])
    NUM_SAMPLES = cfg['data']['sdf_samples']

    # ==========================================================================
    # üõ†Ô∏è QU·∫¢N L√ù TI·∫æN ƒê·ªò (S·ªî ƒêI·ªÇM DANH)
    # ==========================================================================
    LOG_FILE = os.path.join(PROCESSED_DIR, "processed_patients.json")

    processed_pids = set()
    if os.path.exists(LOG_FILE):
        try:
            with open(LOG_FILE, "r", encoding="utf-8") as f:
                processed_pids = set(json.load(f))
        except:
            pass

    # 2. Init Loader & Filter
    loader = DicomLoader(RAW_DIR)
    all_patients = loader.get_all_patient_ids()

    # Ch·ªâ ch·∫°y nh·ªØng ng∆∞·ªùi ch∆∞a c√≥ trong s·ªï ƒëi·ªÉm danh
    target_patients = [p for p in all_patients if p not in processed_pids]
    target_patients.sort()

    if not target_patients:
        print("üéâ Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi. Chuy·ªÉn sang b∆∞·ªõc t·ªïng h·ª£p.")
    else:
        print(f"üöÄ T√¨m th·∫•y {len(target_patients)} b·ªánh nh√¢n M·ªöI. B·∫Øt ƒë·∫ßu x·ª≠ l√Ω...")

    # --- BI·∫æN TH·ªêNG K√ä CHO ƒê·ª¢T CH·∫†Y N√ÄY ---
    stats = {
        "no_nodules": 0,  # B·ªánh nh√¢n kh√¥ng c√≥ nodule
        "consensus_empty": 0,  # Mask r·ªóng/b√©
        "too_complex": 0,  # > 4 b√°c sƒ©
        "too_small": 0,  # Mesh l·ªói
        "success": 0  # S·ªë kh·ªëi u th√†nh c√¥ng
    }

    batch_counter = 0

    # 3. V√≤ng l·∫∑p ch√≠nh (X·ª≠ l√Ω ng∆∞·ªùi m·ªõi)
    if target_patients:
        for pid in tqdm(target_patients, desc="Processing"):
            try:
                # Query & Check
                scan = pl.query(pl.Scan).filter(pl.Scan.patient_id == pid).first()
                if not scan:
                    completed_patients = list(processed_pids)  # ƒê√°nh d·∫•u xong ƒë·ªÉ l·∫ßn sau ko check n·ªØa
                    continue

                nodules = scan.cluster_annotations()
                if not nodules:
                    stats["no_nodules"] += 1
                    processed_pids.add(pid)  # ƒê√°nh d·∫•u xong
                    continue

                # Load Data
                vol_orig, spacing_orig, nodules = loader.load_patient_data(pid)
                if vol_orig is None:
                    continue  # L·ªói load ·∫£nh, ko ƒë√°nh d·∫•u xong ƒë·ªÉ l·∫ßn sau th·ª≠ l·∫°i

                # X·ª≠ l√Ω t·ª´ng kh·ªëi u
                for i, annots in enumerate(nodules):
                    # --- L·ªåC L·ªñI 1 ---
                    if len(annots) > 4:
                        stats["too_complex"] += 1
                        continue

                    # --- L·ªåC L·ªñI 2 ---
                    try:
                        mask_orig, cbbox, _ = consensus(annots, clevel=0.5, pad=10)
                    except:
                        stats["consensus_empty"] += 1
                        continue

                    if np.sum(mask_orig) < 50:
                        stats["consensus_empty"] += 1
                        continue

                    # Preprocessing
                    vol_nodule = vol_orig[cbbox]
                    vol_res, mask_res = resample_volume(vol_nodule, mask_orig, spacing_orig, TARGET_SPACING)
                    vol_norm = normalize_hu(vol_res)
                    roi_vol, roi_mask = crop_roi(vol_norm, mask_res, size=ROI_SIZE)

                    if roi_vol is None: continue

                    # --- L·ªåC L·ªñI 3 ---
                    mesh = mesh_from_mask(roi_mask, spacing=TARGET_SPACING)
                    if mesh is None or len(mesh.vertices) < 10:
                        stats["too_small"] += 1
                        continue

                    # Generate Data
                    points, sdfs = generate_sdf_points(mesh, num_samples=NUM_SAMPLES, roi_size=ROI_SIZE)
                    if points is None: continue

                    # Save
                    file_id = f"{pid}_nodule{i}"
                    np.save(os.path.join(ROI_DIR, f"{file_id}.npy"), roi_vol)
                    np.savez(os.path.join(SDF_DIR, f"{file_id}.npz"), points=points, sdfs=sdfs)

                    stats["success"] += 1

                # Xong b·ªánh nh√¢n n√†y -> Ghi v√†o s·ªï ƒëi·ªÉm danh
                processed_pids.add(pid)
                batch_counter += 1

                # L∆∞u log ƒë·ªãnh k·ª≥ (10 ng∆∞·ªùi/l·∫ßn)
                if batch_counter % 10 == 0:
                    with open(LOG_FILE, "w", encoding="utf-8") as f:
                        json.dump(sorted(list(processed_pids)), f, indent=4)

            except Exception as e:
                # print(f"‚ö†Ô∏è L·ªói {pid}: {e}")
                continue

        # L∆∞u log l·∫ßn cu·ªëi
        with open(LOG_FILE, "w", encoding="utf-8") as f:
            json.dump(sorted(list(processed_pids)), f, indent=4)

        # --- IN B√ÅO C√ÅO CHI TI·∫æT (Ch·ªâ cho ƒë·ª£t ch·∫°y n√†y) ---
        print("\nüìä B√ÅO C√ÅO CHI TI·∫æT (D·ªØ li·ªáu m·ªõi x·ª≠ l√Ω):")
        print(f"‚ùå Kh√¥ng c√≥ nodule: {stats['no_nodules']}")
        print(f"‚ùå Mask r·ªóng/b√©: {stats['consensus_empty']}")
        print(f"‚ùå Qu√° ph·ª©c t·∫°p: {stats['too_complex']}")
        print(f"‚ùå Mesh l·ªói: {stats['too_small']}")
        print(f"‚úÖ TH√ÄNH C√îNG: {stats['success']} m·∫´u")

    else:
        print("\n(Kh√¥ng c√≥ d·ªØ li·ªáu m·ªõi ƒë·ªÉ b√°o c√°o chi ti·∫øt)")

    # ==========================================================================
    # 4. T·ªîNG H·ª¢P & CHIA T·∫¨P (TO√ÄN B·ªò D·ªÆ LI·ªÜU C≈® + M·ªöI)
    # ==========================================================================
    # Qu√©t ·ªï c·ª©ng ƒë·ªÉ l·∫•y t·ªïng s·ªë th·ª±c t·∫ø
    all_npy_files = glob.glob(os.path.join(ROI_DIR, "*.npy"))

    if not all_npy_files:
        print("‚ùå Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu n√†o trong th∆∞ m·ª•c processed.")
        return

    total_samples = len(all_npy_files)
    print(f"\nüì¶ T·ªïng c·ªông: {total_samples} m·∫´u s·∫°ch (C≈© + M·ªõi).")
    print("‚úÇÔ∏è ƒêang chia t·∫≠p d·ªØ li·ªáu...")

    valid_records = [os.path.basename(f).replace(".npy", "") for f in all_npy_files]
    valid_records.sort()

    random.seed(42)
    random.shuffle(valid_records)

    n_train = int(total_samples * 0.7)
    n_val = int(total_samples * 0.1)

    split_dict = {
        "train": valid_records[:n_train],
        "val": valid_records[n_train:n_train + n_val],
        "test": valid_records[n_train + n_val:]
    }

    with open(os.path.join(PROCESSED_DIR, "split_data.json"), "w", encoding="utf-8") as f:
        json.dump(split_dict, f, indent=4)

    print(
        f"‚úÖ ƒê√£ l∆∞u split_data.json (Train: {len(split_dict['train'])}, Val: {len(split_dict['val'])}, Test: {len(split_dict['test'])})")


if __name__ == "__main__":
    main()