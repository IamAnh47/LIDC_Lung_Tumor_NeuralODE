import os
import yaml
import json
import numpy as np
import trimesh
import pylidc as pl
from tqdm import tqdm
from pylidc.utils import consensus

# Import c√°c h√†m x·ª≠ l√Ω t·ª´ src (T√°i s·ª≠ d·ª•ng code ƒë·ªÉ ƒë·∫£m b·∫£o nh·∫•t qu√°n)
from src.data.dicom_loader import DicomLoader
from src.data.preprocessing import resample_volume, crop_roi
from src.data.generation import mesh_from_mask


def main():
    print("üöÄ B·∫ÆT ƒê·∫¶U KI·ªÇM TRA D·ªÆ LI·ªÜU ƒê√É X·ª¨ L√ù (TEST SET)...")

    # 1. Load Config
    if not os.path.exists("configs/config.yaml"):
        print("‚ùå Kh√¥ng t√¨m th·∫•y configs/config.yaml")
        return

    with open("configs/config.yaml", "r") as f:
        cfg = yaml.safe_load(f)

    # ƒê∆∞·ªùng d·∫´n
    RAW_DIR = os.path.abspath(cfg['paths']['raw_data'])
    PROCESSED_DIR = os.path.abspath(cfg['paths']['processed_data'])
    OUTPUT_CHECK_DIR = "test_prepare"  # Th∆∞ m·ª•c y√™u c·∫ßu

    # Tham s·ªë (Ph·∫£i gi·ªëng h·ªát l√∫c prepare_data)
    ROI_SIZE = tuple(cfg['data']['roi_size'])
    TARGET_SPACING = tuple(cfg['data']['target_spacing'])

    # T·∫°o th∆∞ m·ª•c output
    os.makedirs(OUTPUT_CHECK_DIR, exist_ok=True)
    print(f"üìÇ K·∫øt qu·∫£ s·∫Ω l∆∞u t·∫°i: {os.path.abspath(OUTPUT_CHECK_DIR)}")

    # 2. Load danh s√°ch t·∫≠p Test
    split_path = os.path.join(PROCESSED_DIR, "split_data.json")
    if not os.path.exists(split_path):
        print("‚ùå Ch∆∞a c√≥ file split_data.json. H√£y ch·∫°y prepare_data.py tr∆∞·ªõc!")
        return

    with open(split_path, "r") as f:
        splits = json.load(f)
        test_ids = splits["test"]  # Ch·ªâ l·∫•y t·∫≠p test ƒë·ªÉ check

    print(f"üì¶ T√¨m th·∫•y {len(test_ids)} m·∫´u trong t·∫≠p Test.")

    # 3. Kh·ªüi t·∫°o Loader
    loader = DicomLoader(RAW_DIR)

    # 4. V√≤ng l·∫∑p x·ª≠ l√Ω (Re-generate Mesh)
    for file_id in tqdm(test_ids, desc="Generating GT Meshes"):
        try:
            # file_id d·∫°ng: "LIDC-IDRI-0074_nodule0"
            # C·∫ßn t√°ch ra PID v√† Nodule Index
            parts = file_id.split('_nodule')
            pid = parts[0]
            nodule_idx = int(parts[1])

            # Load l·∫°i d·ªØ li·ªáu g·ªëc
            vol_orig, spacing_orig, nodules = loader.load_patient_data(pid)

            if vol_orig is None or nodule_idx >= len(nodules):
                print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu g·ªëc cho {file_id}")
                continue

            # L·∫•y ƒë√∫ng nodule group
            annots = nodules[nodule_idx]

            # --- T√ÅI T·∫†O MESH (Quy tr√¨nh y h·ªát prepare_data) ---

            # A. Consensus
            mask_orig, cbbox, _ = consensus(annots, clevel=0.5, pad=10)

            # Crop Volume g·ªëc (ƒë·ªÉ l·∫•y context n·∫øu c·∫ßn, ·ªü ƒë√¢y ch·ªâ c·∫ßn mask ƒë·ªÉ t·∫°o mesh)
            # Nh∆∞ng resample c·∫ßn c·∫£ 2 ƒë·ªÉ ƒë·ªìng b·ªô
            vol_nodule = vol_orig[cbbox]

            # B. Resample
            _, mask_res = resample_volume(vol_nodule, mask_orig, spacing_orig, TARGET_SPACING)

            # C. Crop ROI (ƒê·ªÉ ƒë√∫ng k√≠ch th∆∞·ªõc 64x64x32)
            # L∆∞u √Ω: Ta c·∫ßn crop mask theo ƒë√∫ng logic ƒë√£ l√†m v·ªõi ·∫£nh
            # ƒê·ªÉ ƒë∆°n gi·∫£n, ta truy·ªÅn dummy volume v√†o h√†m crop_roi
            dummy_vol = np.zeros_like(mask_res, dtype=np.float32)
            _, roi_mask = crop_roi(dummy_vol, mask_res, size=ROI_SIZE)

            if roi_mask is None:
                print(f"‚ö†Ô∏è Mask r·ªóng sau khi crop: {file_id}")
                continue

            # D. T·∫°o Mesh (Ground Truth)
            # ƒê√¢y l√† h√†m d√πng Marching Cubes


            #[Image of marching cubes algorithm]

            mesh = mesh_from_mask(roi_mask, spacing=TARGET_SPACING)

            if mesh:
                # E. Export
                # CƒÉn gi·ªØa ƒë·ªÉ d·ªÖ xem tr√™n Blender
                mesh.apply_translation(-mesh.centroid)

                save_name = f"{file_id}_GT.obj"
                mesh.export(os.path.join(OUTPUT_CHECK_DIR, save_name))

                # (T√πy ch·ªçn) L∆∞u th√™m file Point Cloud t·ª´ .npz ƒë·ªÉ so s√°nh
                # copy_point_cloud(PROCESSED_DIR, file_id, OUTPUT_CHECK_DIR)
            else:
                print(f"‚ö†Ô∏è Kh√¥ng t·∫°o ƒë∆∞·ª£c Mesh cho {file_id}")

        except Exception as e:
                print(f"‚ùå L·ªói khi x·ª≠ l√Ω {file_id}: {e}")
    print("\n‚úÖ HO√ÄN T·∫§T! H√£y m·ªü th∆∞ m·ª•c 'test_prepare' v√† k√©o file .obj v√†o Blender.")


def copy_point_cloud(processed_dir, file_id, output_dir):
    """
    H√†m ph·ª•: Tr√≠ch xu·∫•t ƒëi·ªÉm t·ª´ file .npz ra file .obj (d·∫°ng ƒë√°m m√¢y ƒëi·ªÉm)
    ƒë·ªÉ xem model th·ª±c s·ª± 'nh√¨n th·∫•y' nh·ªØng ƒëi·ªÉm n√†o.
    """
    try:
        npz_path = os.path.join(processed_dir, "sdfs", f"{file_id}.npz")
        data = np.load(npz_path)
        points = data['points']
        sdfs = data['sdfs']  # Ho·∫∑c 'values' t√πy t√™n ƒë·∫∑t

        # L·ªçc l·∫•y c√°c ƒëi·ªÉm b·ªÅ m·∫∑t (SDF g·∫ßn 0) ƒë·ªÉ visualize cho nh·∫π
        # Ho·∫∑c xu·∫•t h·∫øt
        pcd = trimesh.points.PointCloud(points)

        # CƒÉn gi·ªØa (C·∫ßn kh·ªõp v·ªõi mesh ·ªü tr√™n n·∫øu mu·ªën ch·ªìng h√¨nh)
        # pcd.apply_translation(-pcd.centroid)

        pcd.export(os.path.join(output_dir, f"{file_id}_POINTS.obj"))
    except:
        pass


if __name__ == "__main__":
    main()